In this lecture, we will look at CPU Scheduling Algorithms. We had seen in operating systems that a scheduler present would choose a particular process from the ready queue and that process is assigned to run in the processor. Now the question that we were going to analyze now is how should this scheduler choose the next process? Essentially, how should the scheduler choose a process to run on the CPU from the existing queue of ready processes?

Now to analyze this, we first look at execution phases of a process. Now any program has been known to have two phases of execution; one is when it is actually executing instructions which is known as a CPU burst, while the other is when it is blocked on an I/O or not doing any operations, so in this particular case, the CPU is idle. Thus, as we look with over time, a particular process would have some amount of CPU burst in which it would execute instructions on the processor, then it would have an some idle time in which it is waiting for a I/O operation. Then there would be a burst of CPU time and so on. Thus, there is always this inter-leaving between CPU burst and idle time that is a waiting for an operation.

So, based on this cases of execution one could classify processes into two types. One is the I/O bound processes, while the other is the CPU bound process. Why do you make this distinction between I/O bound and CPU bound processes? Essentially, this is from a scheduling perspective, we would like to give I/O bound processes a higher priority with which they are allocated the CPU. Essentially, we want that I/O bound processes wait lesser time for the CPU compared to CPU bound processes.

So, why is this required, we can take with an example. So suppose we are using a word processor such as a notepad or Vim and we are giving this I/O bound process a very low priority. Now, suppose a user presses a key, because it has a low priority it does not get the CPU to very often and therefore, it would take some time before that key pressed by the user appears onto the screen. So, this may be quite uncomfortable for the user, therefore we would like to give the I/O bound process such as the word processor higher priority with which it will get the CPU, so that the user interaction with the CPU becomes more comfortable.

On the other hand, if you look at CPU bound processes, we could give it a lower priority. Now for instance, if you take one of these applications, the CPU bound applications like for instance say let us say ‘gcc’ that is compiling a program and let us say you are compiling a large program which takes 5 minutes. Now it will not effect this user much if the time taken to compile that particular program increases from 5 minutes to say 5.5 minutes. Thus, a CPU bound processes could work with a lower priority. This classification between I/O bound and CPU bound is not a rigid classification, that is a process could be an I/O bound process at one time, and after some time it could behave like a CPU bound process.

So, to take an example of a process which behaves both like an I/O bound as well as CPU bound, you could take for instance Microsoft excel. When we are actually entering data into the various cells in excel, it acts as I/O bound process. So, it behaves like an I/O bound process with small CPU burst and large times of I/O cycles. While on the other hand when you are actually computing some statistic on the data entered, Excel will behave like a CPU bound process, where there is a large portion of CPU activity or the time taken to actually operate on that particular data. Now, let us come back to the question about how the CPU scheduler should choose from the queue of ready processes, the next process to execute in the CPU. There could be several ways in which the scheduler could makes this choice; essentially, there could be several CPU scheduling algorithms which would look into the queue and make a particular decision.

So in order to compare these various scheduling algorithms, operating systems text books or operating systems research defines several scheduling criteria. So, these criteria could be used to actually to compare various scheduling algorithms to see the advantages and disadvantages of each of them. So let us go through each of these scheduling criteria one by one. The first scheduling criteria is the CPU utilization. The scheduling algorithm should be designed in such a way so as to maximize CPU utilization. In other words, the CPU should be idle as minimum time as possible. The next criteria, we will look at is the throughput. Essentially, scheduling algorithms would try to complete as many processes as possible per unit time. A third criteria is the turnaround time and this criteria is looked at from a single process perspective. So, turnaround time is defined as the time taken for a single process from start to completion. The fourth criteria is response time. So, this is defined as the time taken from the point that when the process enters into the ready queue to the point when the process goes into the running state, that is the time taken from the instant the process enters the ready queue to the time the CPU begins to execute instructions corresponding to that process. Another criteria, is the waiting time. Now this criteria, is based on the time taken by a process in the ready queue. Now as we know processes ready to run are present in the ready queue and it is required that they do not wait too long in the ready queue. So, scheduling algorithms could be designed in such a way that the waiting time or the average waiting time in the ready queue is minimized. The final criteria we will see now is fairness. The scheduler should ensure that each process is given a fair share of the CPU based on some particular policy. So, it should not be the case that some process, for instance, takes say 90 percent of the CPU while all other processes just get round 10 percent of the CPU.

So, all these criteria need to be considered while designing a scheduling algorithm for an operating system. A single scheduling algorithm will not efficiently be able to cater to all these criteria simultaneously. So, therefore, scheduling algorithms are therefore designed for to meet a subset of these criteria. For instance, if you consider real time operating system, the scheduling algorithm for that system would for instance be designed to have minimum response time; other factors such as CPU utilization and throughput may be of secondary importance. On the other hand, desktop operating system like a Linux will be designed for fairness, so that all applications running in the CPU or in the system are given a fair share of the CPU. Criteria such as response time may be less important from that perspective.

So, we will now look at several scheduling algorithms starting from the simplest one that is the first come first serve scheduling algorithm and go to more complex scheduling algorithms as we proceed. So let us look at the First Come First Serve of the FCFS scheduling algorithm. The basic scheme in this case is that the first process that requests the CPU would be allocated the CPU or in other words, the first process which enters into the ready queue would be allocated the CPU. So, this is a non preemptive scheduling algorithm which means that the process once allocated the CPU will continue to execute in the CPU until its burst cycle completes.

Let us see this with an example. Let us say we have a system with four processes running; the processes are label P 1, P 2, P 3, P 4 and they have an arrival time, so the arrival time is present in the second column. So the arrival time is defined as the time when these processes enter into the ready queue. So, for this particular very simple example, so we will consider that all processes enter the ready queue at the same time that is at the 0th time instant. The third column is the CPU burst time, so it gives the amount of CPU burst for each process. For instance, P 1 has a CPU burst of 7 cycles; P 2 has a burst of 4 cycles. Thus, this particular table we have like four processes, which all enter simultaneously into the ready queue at the time instants 0 and they have different CPU burst time, for instance P 1 has 7 cycles, P 2 - 4 cycles, P 3 - 2 cycles and P 4 - 5 cycles. Now, we will see how these four processes get scheduled into the CPU or how these four processes get allocated the CPU. Since, all of these processes arrive at the same time, the scheduler does not actually have a choice to make. So, he would pick randomly a particular ordering. For instance, let us say the scheduler picks P 1 to run, so P 1 runs for 7 cycles and when it completes, the scheduler picks P 2 and P 2 runs for 4 cycles. After P 2 completes its burst, and P 3 executes in the CPU for 2 cycles; and then finally, P 4 is scheduled in to execute for 5 CPU cycles. So, this is represented by a Gantt chart. So, a Gantt chart is a horizontal bar chart developed as a production tool in 1917 by Henry L Gantt who was an American engineer and a Social scientist. So, essentially in a Gantt chart we have like several blocks over here and each block represents a cycle of execution. So, for instance, P 1 executes for 7 cycles, so it has like 7 blocks – 1, 2, 3, 4, 5, 6, 7, P 2 then executes for 4 cycles, so it’s given like 4 blocks. Then P 3 executes for 2 cycles, so it is given 2 blocks and finally, P 4 executes for 5 cycles, so it is given 5 blocks. So, we could compute the average waiting time for this particular case. We see that process P 1 enters into the ready queue at the instant 0 and immediately gets to execute in the CPU. Thus, it does not have to wait at all. The second process P 2 arrives also at 0 that is also arrives at this point but gets to execute only after P 1 executes that is after 7 cycles; thus, its wait time is for process P 2 is 7. Similarly, process P 3 which also enters in the 0th cycle gets to execute only after process P 1 and P 2 completes. In other words, it has to wait 11 cycles and the fourth process in a similar way needs to wait for 13 cycles. Therefore, the average waiting time in this particular case is 7.75 cycles. Now, we can look at an other scheduling criteria, which is the average response time. So, average response time in this case is the same as the average waiting time. So the average response time is the time taken for a particular process to begin executing in the CPU minus the time it actually enters into the ready queue. So, for instance, P 2 enters into the ready queue at this instant, but begins to execute in the CPU only after 7 cycles. So, therefore, the response time for process P 2 is 7. Similarly, process P 3 has a response time of 11 because it has waited for 11 cycles to actually begin executing in the CPU. So, on average, the average response time is 7.7 5 just like the average waiting time. Now, one characteristic of the FCFS scheduling algorithm is that the order of scheduling matters. In the previous slide that is in this slide we had assume that P 1 executes then P 2 executes then P 3 and then P 4, and we have got average waiting time and average response time of 7.75 cycles. Now, suppose we just change the ordering and let us say the ordering is now as follows that P 2 executes then P 3 executes then P 4 and finally P 1. In such a case, if we compute the average waiting time it we see that it gets reduce from 7.75 cycles to 5.25 cycles. Similarly, if you compute the average response time, you will see that the response time is also 5.25 cycles. In a similar way, you could compute the other criteria, which we are mentioned over here and you will be able to actually see the difference with the two ordering schemes. Another characteristic of the FCFS scheduling algorithm is the Convoy effect. Essentially, in this particular case we see that all processes wait for one big process to get off the CPU. So, for instance, in this case, until or rather even though all processes enter the CPU at the same time, all processes have to wait for P 1 to complete only then they could be scheduled.

So, if we have a large process over here for instance, we have a process P 1 instead of burst time of 7 takes a burst time of say 100, all other processes P 2, P 3 and P 4 would wait for 100 block cycles before they actually be able to get to execute in the CPU. So, this is a huge drawback of FCFS scheduling scheme. However, FCFS scheduling algorithm have several advantages. For the first thing is it is extremely simple. So the scheduling algorithm could complete very quickly and therefore, the time taken by the scheduling algorithm will be very less, and you would end up with very less context delays while changing the contexts. Another advantage of the FCFS scheduling algorithm is that it is fair.

As long as no process hogs the CPU, every process will eventually run; or in other words, as long as every process terminates at some point, every other process in the ready queue will eventually get to execute in the CPU. Now the drawback or the disadvantage of the FCFS schedulers as we have seen is that the waiting time depends on the arrival order. So, we have seen the example in the previous slides. Another disadvantage is that short processes are stuck in the ready queue waiting for long processes to complete; or rather, this is the convoy effect that we have just looked at.

Now, let us look at another scheduling algorithm know as the Shortest Job First scheduling algorithm. In this particular scheduling algorithm, the job or the process with the shortest CPU burst time is scheduled before the others. Now if you have more than one process with the same CPU burst time then standard FCFS scheduling is used. There are two variants of the shortest job first scheduling algorithm. The first is the no preemption variant, while the second one is the shortest job first with preemption. Now in the SJF with no preemption, the process will continue to execute in the CPU until its CPU burst completes.

In the second variant with preemption, it may be possible that the process which may get preempted, when a new process arrives into the ready queue. We will see more on this later, but first we will start with the shortest job first variant with no preemption.

So let us take the same example that we have seen in the previous case, we had their four processes P 1 to P 4 and all of them are arriving at the instant 0, that is at instant 0 these four processes P 1 to P 4 get into the ready queue. And each of these processes have a different CPU burst time that is 7, 4, 2 and 1 respectively. Now in the first instant, the CPU scheduler will look at the various burst times and find the one which is minimum. So, in this case we see that P 4 has the minimum CPU burst time, so that were scheduled first. So, first the process P 4 gets scheduled until it completes, in this particular case it completes in 1 cycle. Then among the remaining three, we see that P 3 has the lowest CPU burst time. So, process P 3 gets scheduled and completes till and executes till it completes its burst. Then P 2 gets scheduled because it has a burst time of 4, while P 1 has a higher burst time of 7. And finally, P 1 gets to execute till completion. Now, the average wait time, if we compute this is 2.75, while the average response time is also 2.75 as in the wait time case. Now let us look at another example of shortest job first without preemption. So, we will take the same four processes P 1 to P 4 and each of these processes have the same burst time as before that is 7, 4, 2 and 1 respectively. However, they arrive at different instants that is the moment the instant in which they enter into the ready queue would be different. So, P 1 enters at the 0th instant, P 2 in the second instant, P 3 at the fourth instant and P 4 in the 7th instant. Now, this is a slight modification in the Gantt chart, where in addition to showing which process is executing in the CPU, it also shows the order in which processes arrive. It shows that the P 1 arrives first, then P 2 in the second instant, then P 3 and finally P 4 in the seventh instant. Now, when the scheduler begins to execute at this particular instant, the only process that has arrived at this particular point is P 1 therefore, it schedules P 1 to execute. So, P 1 executes for its entire burst that is of 7 cycles, and then at this particular cycle, the scheduler enters again or a scheduler executes again and this time it has got three processes to choose from all P 2, P 3 as well as P 4 have arrived in the ready queue. And out of them P 4 has the shortest burst time therefore, it is chosen for execution. Therefore, P 4 executes in the CPU, and then P 3 because P 3 has a shorter burst time than P 2 and finally, P 2 gets executed. So, if we compute the average wait time, we see that it is 3 cycles.

So the advantages of the shortest job first scheduling algorithm is that is Optimal. It will always give you the minimum average waiting time. And as a result of this, the average response time also decreases. The main disadvantages of the SJF scheduling algorithm is that it is not practical; essentially, it is very difficult to predict what the burst time would be. So another drawback of the SJF scheduling algorithm is that some jobs may gets starved; essentially if you have a process which has an extremely long CPU burst time, then it may never get a chance to actually execute in the CPU.

Now, we will look at the shortest job first scheduling algorithm (SJF) with preemption. So, this is also the shortest remaining time first. So the basic idea in this algorithm is that if a new process arrives into the ready queue, and this process has a shorter burst time than the remaining of the current process, then there is a context switch and the new process gets scheduled into the CPU. This further reduces the average waiting time as well as the average response time. However, as in the previous case that is the shortest job first with no preemption here also it is not practical. So let us understand more on this with an example. So let us take the same example of 4 processes with burst time 7, 4, 2 and 1; and arrival times at 0, 2, 4 and 7, will develop the Gantt chart as the time processes. So, at the instant 0, the only process which is present is P 1; and therefore, the scheduler has no choice, but to schedule P 1 onto the CPU. Thus P 1 would execute for 2 clock cycles. Now after the second clock cycle the process P 2 has entered into the ready queue. Now you see that P 2 has a burst of 4 cycles. However, P 1 has a remaining burst of 5 cycles. So, what we mean by this is that out of the CPU burst time of 7 cycles, P 1 has completed 2 cycles. So, what remain is 5. Now the scheduler will see that P 1 has 5 cycles, which is greater than P 2, which has burst of 4 cycles. Therefore, it will do a context switch and schedule P 2 to run. So, P 2 will run for 2 clock cycles and then P 3 arrives at the fourth clock instant. So, at this particular instant, the scheduler will find out that P 3 has a burst time of 2 cycles, while P 2 has a remaining burst time of 2 cycles; we achieve 2 because out of the 4 cycle burst time for P 2, it has completed 2. So, what remains is 2 more cycles. Since P 2 the old process which is running on the CPU has a 2 cycle remaining burst time, and P 3 the new process also has 2 cycles burst time therefore, there is no preemption and P 2 will continue to execute. Now after P 2 completes, in this particular case, P 3 executes for 2 cycles, and then P 4 enters now after which if you will verify will not cause any preemption. So, after P 3 completes, the scheduler decides to run either P 4 or P 1. So, we see that P 4 has a burst cycle of 1 while P 1 has a remaining burst cycle of 5, therefore the scheduler will decide to choose P 4 over P 1. So, P 4 runs on the CPU and after it completes P 1 will executes for its remaining burst time. So, if you compute the average wait time, you see that it reduces to 2.5, while the average response time reduces considerably to 0.75.

However, as we mentioned before just like the shortest job first, this scheduling algorithm is also not feasible to be implemented in practice, because it is very difficult to actually identify what the burst time of a process is and even more difficult to identify what the remaining burst time of the process would be.

Now we will look at another scheduling algorithm known as the Round Robin Scheduling algorithm. So, essentially with the round robin scheduling algorithm, a process runs for a time slice that is a process executes for a time slice and when the time slice completes it is moved on to the ready queue. So, in order to achieve this round robin scheduling algorithm which is also a preemptive scheduling algorithm; now in order to achieve the round robin scheduling, we need to configure the timer in the system to interrupt the CPU periodically. At every timer interrupt, the kernel would preempt the current process and choose another process to execute in the CPU.

Let us discuss the round robin scheduling algorithm with an example. So, one difference with respect to the other scheduling algorithms that we have seen so far is the notion of time slice. So, this is the Gantt chart. So, especially see that periodically in this case, with a period equal to 2 that is we have keeping a time slice equal to 2, there is a timer interrupt that occurs and the timer interrupt would result in the scheduler being run and potentially another process being scheduled into the CPU. So, a data structure which is very useful in implementing the round robin scheduling algorithm is the FIFO. This particular FIFO stores the processes that need to be executed next into the CPU. For example, in this particular case, P 2 is at the top of the FIFO, so it is a next process which gets executed in the FIFO. So, P 2 gets executed over here. So, for example, we will still consider the 4 processes as we have done before that is P 1 to P 4 and we will assume that all of them arrive at the instant 0 and go into the FIFO of the ready queue and they have burst times of 5, 4, 2 and 3 respectively. So let us say for discussion that the scheduler starts off with this particular order P 1, P 2, P 3 and P 4 and it first chooses to execute P 1. So, P 1 executes for 2 cycles and then there is a interrupt which occurs leading to a context switch and then the top of the FIFO in this particular case P 2 get scheduled into the CPU, while P 1 gets pushed into the FIFO. So, P 2 then executes for 2 cycles until the next timer interrupt in which case the time slice of 2 cycles completes and then it gets pushed into the FIFO. So, P 2 now is at the bottom of the FIFO, while P 3 which is at the top of the FIFO get scheduled to run. So, in this way, every two cycles a new process may get scheduled into the CPU and execute. So, if we compute the average waiting time. So, in this particular case, we will see that the average waiting time and the average response time is different. So, what is the average waiting time for P 1? So, the P 1 executes 2 cycles here, 2 cycles here and completes execution over here. So, it waits in the in the ready queue in the remaining of the cycles. So the number of cycles it waits is 1, 2, 3, 4, 5, 6, 7, 8, 9. So, P 1 waits for 9 cycles; now P 2 waits for 8 cycles – 1, 2, 3, 4, 5, 6, 7, 8; P 3 for 4 cycles and P 4 for 10 cycles. So the average waiting time is 7.75 cycles. Now, to compute the average response time as we have defined it before, the response time is the time the process enters into the ready queue to the time it begins to execute in the CPU, that latency would be the response time. So, P 1 for instance has a response time of 0 because it enters into the ready queue or enters into the FIFO and gets executed immediately. P 2 on the other hand enters in the 0th cycle, but gets to execute only after 2 cycles, so it has a response time of 2. Similarly, P 3 enters at 0 but executes only at this point. So, at rather at this instant therefore, it has a response time of 4 and P 4 has a response time of 6 therefore, the average response time is 3. Now the number of context switches that occur is 7. So, in this case, 1, 2, 3, 4, 5, 6 and a context switch occurs over here because the process P 4 is existing out and P 1 gets continues to execute. So the numbers of context switches over here are 7 that is 1, 2, 3, 4, 5, 6 and the seventh context switch occurs over here then P 4 exits and P 1 gets switched into the CPU. Now let us take another more complex example of round robin scheduling, where we also have arrival times which are not the same. So, P 1 is arriving at the 0th instant; P 2 at the second, P 3 and P 4 at the third and ninth respectively. So, we can similarly draw the Gantt chart and the states of the FIFO for this case. So to start with, in the 0 instant, the only process which has arrived is P 1 and therefore, P 1 executes for 2 cycles. And at this particular point, when the timer interrupt occurs, no other process is present as yet, therefore P 1 will continue to execute for another 2 cycles for another time slice. However, in this time slice, we have two processes which have entered into the ready queue; these are the process P 2 and P 3. So, P 2 arrives at this interval while P 3 arrives at this interval and they get added into the FIFO. So, at the second time slice completion, there is a context switch and P 2 gets scheduled into the CPU to execute while P 1 which was executing will then go into the FIFO. So, P 2 executes for 2 cycles, then P 3 executes for 2 cycles, then P 1 executes for 2 cycles and at that time P 4 has arrived and gets added into the FIFO. Now, we have three processes P 1, P 2 and P 4 and these get schedule to run for a period of time. So the average waiting time in this case is 4.75, while the average response time is 2. So, how is the average waiting time 4.75, it means that process P 1 has waited for 7 cycles so, before it completes. So that is 1, 2, 3, 4, 5, 6 and 7. While process P 2 has waited for 6 cycles, so it is 1, 2, 3, 4, 5, 6; process 3 has waited for 3 cycles, and process 4 has waited for 3 cycles. The average response time can be verified to be equal to 2 and the number of context switches was as before 7.

Now let us take the same example as we have done before, but with a time slice of one that is we have reduced the time slice from 2 and we have made it 1. So, we see that in every instant we have a timer interrupt and potentially context switch that can occur. If we compute the average waiting time and average response time for this particular case we see that the average response time in particular has reduced. Why has this occurred is because once a process has entered into the queue it has to wait for lesser cycles before it gets scheduled into the CPU. On the other hand, the number of context switches has increased from 7 to 11.

Since we are having timer interrupts which are more frequent therefore, there is more likely that a context switch will occur. Now, if we take the same example, but with the time slice of 5 instead of 1, and if we compute the average waiting time and average response time, we see that the response time increases considerably to 2.75 while the number of context switches reduces quite a bit to 4. On the other hand, we see that the scheduling begins to behave more and more like the first come first serve. The response time is bad because due to the large time slice the scheduling behaves more and more like the first come first serve which we know has a bad response time. So, from all this examples that we have seen so far we seen we can conclude that the duration of a time slice is very critical. So, it effects both the response time as well as the number of context switches. So, essentially, if we have a time slice which is of a very short quantum, the advantage is that processes need not wait too long in the ready queue before they get schedule into the CPU. Essentially this means that the response time of the process would be very good or it would have a less response time. On the other hand, having a short time slice is bad, because we would have very frequent context switches. And as we seen before context switches could have considerable over heads. Therefore, it degrades the performance of the system.

A long time slice or a long quantum has a drawback that processes no longer appear to execute concurrently, it appears more like a first come first serve type of scheduling algorithm and so this again in turn may degrade system performance. So, typically in a modern day operating systems the time slice duration is kept anything from 10 milliseconds to 100 milliseconds. So, xv6 programs, programs timers to interrupt every 10 milliseconds.

So the advantage of the round robin scheduling algorithm is as follows. The algorithm is fair because each process gets a fair chance to run on the CPU. The average wait time is low especially when the burst times vary and the response time is very good. On the other hand, the drawbacks of the round robin scheduling algorithm are as follows; there is an increase number of context switching that occurs and as we have seen before context switching has considerable over heads. And the second drawback is that the average wait time is high especially when the burst times have equal lengths.

The xv6 scheduling policy is a variant of the round robin scheduling policy; the source code is shown over here. So, essentially what the xv6 scheduler does is that it parses through the ptable array. So, we have seen ptable before, which is an array of procs and the scheduler parses through this particular array and finds the next process that is runnable and invokes the switch. So, this is where you invoke the context switch. So, every time the scheduler executes, the next process in this array that is runnable gets scheduled to execute.
